{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from model import ResNet\n",
    "\n",
    "import numpy as np\n",
    "from main import ChunkSampler, get_param_count, check_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/cifar-10-python.tar.gz to ./dataset\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4)])\n",
    "transform_normalize = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "# get CIFAR-10 data\n",
    "NUM_TRAIN = 45000\n",
    "NUM_VAL = 5000\n",
    "batch_size = 128\n",
    "cifar10_train = dset.CIFAR10('./dataset', train=True, download=True,\n",
    "                                transform=T.Compose([transform_augment, transform_normalize]))\n",
    "loader_train = DataLoader(cifar10_train, batch_size=batch_size,\n",
    "                            sampler=ChunkSampler(NUM_TRAIN))\n",
    "cifar10_val = dset.CIFAR10('./dataset', train=True, download=True,\n",
    "                            transform=transform_normalize)\n",
    "loader_val = DataLoader(cifar10_train, batch_size=batch_size,\n",
    "                        sampler=ChunkSampler(NUM_VAL, start=NUM_TRAIN))\n",
    "cifar10_test = dset.CIFAR10('./dataset', train=False, download=True,\n",
    "                            transform=transform_normalize)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "res_option = 'A'\n",
    "weight_decay = 0.0001\n",
    "use_dropout = False\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count: 465290\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = ResNet(n, res_option=res_option, use_dropout=use_dropout)\n",
    "\n",
    "param_count = get_param_count(model)\n",
    "print('Parameter count: %d' % param_count)\n",
    "\n",
    "# use gpu for training\n",
    "if not torch.cuda.is_available():\n",
    "    print('Error: CUDA library unavailable on system')\n",
    "    \n",
    "global gpu_dtype\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "model = model.type(gpu_dtype)\n",
    "\n",
    "# setup loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "# train model\n",
    "SCHEDULE_EPOCHS = [50, 5, 5] # divide lr by 10 after each number of epochs\n",
    "#     SCHEDULE_EPOCHS = [100, 50, 50] # divide lr by 10 after each number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for num_epochs in SCHEDULE_EPOCHS:\n",
    "    print('Training for %d epochs with learning rate %f' % (num_epochs, learning_rate))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                            momentum=0.9, weight_decay=weight_decay)\n",
    "    for epoch in range(num_epochs):\n",
    "        check_accuracy(model, loader_val)\n",
    "        print('Starting epoch %d / %d' % (epoch+1, num_epochs))\n",
    "        train(loader_train, model, criterion, optimizer)\n",
    "    learning_rate *= 0.1\n",
    "\n",
    "print('Final test accuracy:')\n",
    "check_accuracy(model, loader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
